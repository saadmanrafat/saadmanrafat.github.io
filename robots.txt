# robots.txt for saadman.dev
User-agent: *
Allow: /
Disallow: /search
Disallow: /assets/raw/
Disallow: /private/
Disallow: /cgi-bin/
Disallow: /temp/
Disallow: /draft/

# Prevent crawling of specific file types
Disallow: /*.pdf$
Disallow: /*.doc$
Disallow: /*.docx$

# Prevent crawling of any backend routes
Disallow: /api/

# Prevent parameter-based URLs
Disallow: /*?*

# Sitemap
Sitemap: https://saadman.dev/sitemap.xml

# Add crawl delay to prevent server overload
Crawl-delay: 5